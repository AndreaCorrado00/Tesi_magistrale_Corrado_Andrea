}
# Calculate percentage of NaNs in each variable
nan_percentages <- calculate_nan_percentage(train)
# Print the percentage of NaNs for each variable
print(nan_percentages)
# Variables names are passed, with training and test set, to the function specified above.
#updated numerical variables names onto using mean-imputation
numerical_col_names<-c("age", "bmi", "ferritin","crprot","hemoglobin","gluc","ldl","hdl","trig","hba1c","systolic_bp","diastolic_bp")
# mode for other variables
names_var<-colnames(train)
categorical_col_names<-names_var[-which(names_var %in% c(numerical_col_names,'outcome'))]
imputed_data<-impute_mean_mode(train,test,numerical_col_names,categorical_col_names)
norm_data<-normalize_min_max(imputed_data$imputed_train,imputed_data$imputed_test)
train_imp_norm<-norm_data$norm_train
test_imp_norm<-norm_data$norm_test
nan_percentages <- calculate_nan_percentage(train_imp_norm)
print(nan_percentages)
nan_percentages <- calculate_nan_percentage(test_imp_norm)
print(nan_percentages)
log_full_model<-glm(outcome~., data = train_imp_norm, family="binomial")
print(summary(log_full_model))
# here probability are saved
full_model_prediction<-predict(log_full_model, type="response", newdata = test_imp_norm)
# funtion to handle the roc curve even in the next phases
roc_evaluation<-function(prob_prediction, plotting=TRUE,printing=FALSE, auc_out=FALSE){
library(pROC)
roc_full_model<-roc(response=test_imp_norm$outcome,predictor=prob_prediction)
#plotting roc curve
x<-1-roc_full_model$specificities
y<-roc_full_model$sensitivities
if(plotting){
plot(x,y,main=c('ROC curve with AUC of',toString(round(roc_full_model$auc,digits=3))),
xlab='1-specificity',ylab='sentivity',type='l')+lines(c(0,1),c(0,1),col="red")
}
if(printing){
cat('AUC of the full model', toString(round(roc_full_model$auc,digits=3)))
}
if(auc_out){
return(round(roc_full_model$auc,digits=3)) }
}
roc_evaluation(full_model_prediction,plotting=TRUE, printing = FALSE, auc_out=FALSE)
auc_full_model<-roc_evaluation(full_model_prediction,plotting=FALSE, printing = FALSE, auc_out=TRUE)
AUC_output<-rep(NA,200)
set.seed(0)
for(i in 1:200){
train_boo_c1_ind<-sample(which(train_imp_norm$outcome==1),length(which(train_imp_norm$outcome==1)),replace = TRUE)
train_boo_c0_ind<-sample(which(train_imp_norm$outcome==0),length(which(train_imp_norm$outcome==0)),replace=TRUE)
train_boo_ind<-sort(c(train_boo_c1_ind,train_boo_c0_ind))
train_boo<-train_imp_norm[train_boo_ind,]  # will act as internal train test
set_oob<-train_imp_norm[-train_boo_ind,]   # will act as internal test set
boo_full_model<-suppressWarnings(glm(outcome~., data = train_boo, family="binomial"))
boo_prob_prediction<-predict(boo_full_model, type="response", newdata = set_oob)
roc_boo_full_model<-roc(response=set_oob$outcome,predictor=boo_prob_prediction,quiet=TRUE)
AUC_boo<-roc_boo_full_model$auc
AUC_output[i]<-AUC_boo
}
boxplot(AUC_output,main='auc bootstrap')
# mean and quantile
mean_auc_full<-mean(AUC_output)
# 95% quantile
CI_low<-quantile(AUC_output,0.025)
CI_upper<-quantile(AUC_output,0.975)
cat('AUC of the full model', toString(auc_full_model),'\n----Bootstrap----','\nmean full model AUC',toString(round(mean_auc,digits=3)), '\nCI upper bound', toString(round(CI_upper,digits=3)),'\nCI lower bound', toString(round(CI_low,digits=3)))
summary(final_model_backward)
# Setting the working directory
setwd("D:/Desktop/ANDREA/Universita/Magistrale/Anno Accademico 2023-2024/2. Medical Big Data/Final Project")
rm(list=ls())
load("data_final_project.RData")
# inspecting the database
summary(data)
# performing stratified splitting
# indices
pos_ind<-which(data$outcome==1)
neg_ind<-which(data$outcome==0)
set.seed(0)
# random splitting the data stratified
tr_ind_pos<-sample(pos_ind,size=round(0.8*length(pos_ind)),replace = FALSE)
tr_ind_neg<-sample(neg_ind,size=round(0.8*length(neg_ind)),replace = FALSE)
ind_train<-sort(c(tr_ind_neg,tr_ind_pos))
ind_test<-1:nrow(data)
ind_test<-ind_test[ - ind_train]
# train and test dataset
train<-data[ind_train,]
test<-data[ind_test,]
calc_perc<-function(df){
perc_0_tot<-nrow(df[df$outcome==0,])/nrow(df)*100
perc_1_tot<-nrow(df[df$outcome==1,])/nrow(df)*100
cat('    - Positive (diabetic) class:',toString(round(perc_1_tot),digits=3),'%\n')
cat('    - Negative (no diabetic) class:',toString(round(perc_0_tot),digits=3),'%\n')
}
cat('Whole df\n')
calc_perc(data)
cat('Train df\n')
calc_perc(train)
cat('Test df\n')
calc_perc(test)
rule_tumb_var<-nrow(train[train$outcome==1,])/ncol(train)
if(rule_tumb_var>=10){
cat('Rule of thumb verified: ',toString(round(rule_tumb_var, digits = 2)))
}
numerical_col_names<-c("age", "bmi", "waist", "ferritin","crprot","hemoglobin","gluc","ldl","hdl","tot_chol","trig","hba1c","systolic_bp","diastolic_bp")
num_train_df<- train[, numerical_col_names]
library(ggplot2)
col_names<-colnames(num_train_df)
for(i in 1:ncol(num_train_df)){
g<-ggplot(data=num_train_df, aes(y=num_train_df[,col_names[i]]))+
geom_boxplot(na.rm=TRUE)+
labs(title = toString(col_names[i]))
print(g)
}
library(corrplot)
corrplot(corr=cor(na.omit(num_train_df)), method = 'number')
train$waist<-NULL
test$waist<-NULL
train$tot_chol<-NULL
test$tot_chol<-NULL
impute_mean_mode <- function(train,test,var_by_mean,var_by_mode) {
mean_var_ind<-which(colnames(train) %in% var_by_mean)
for(k in mean_var_ind){
mean_var<-mean(train[,k],na.rm=TRUE)
nan_id<-which(is.na(train[,k]))
train[nan_id,k]<-mean_var
ind_na_test<-which(is.na(test[,k]))
test[ind_na_test,k]<-mean_var
}
mode_var_ind<-which(colnames(train) %in% var_by_mode)
for(k in mode_var_ind){
freq<-table(train[,k])
all_values<-sort(unique(train[,k]))
ind_mode<-which.max(freq)
mode_train_var<- all_values[ind_mode]
#imputing
nan_id<-which(is.na(train[,k]))
train[nan_id,k]<-mode_train_var
ind_na_test<-which(is.na(test[,k]))
test[ind_na_test,k]<-mode_train_var
}
return(list(imputed_train=train,imputed_test=test))
}
normalize_min_max<-function(train,test,var_to_norm){
ind_num<-which(sapply(train,is.factor)==FALSE)
for(k in ind_num){
min_by_k<-min(train[,k])
max_by_k<-max(train[,k])
# assign new variable values
delta_var<-max_by_k-min_by_k
train[,k]<-(train[,k]-min_by_k)/delta_var
test[,k]<-(test[,k]-min_by_k)/delta_var
}
return(list(norm_train=train,norm_test=test))
}
# Load required packages
library(dplyr)
# Function to calculate percentage of NaNs in each variable
calculate_nan_percentage <- function(data) {
nan_percentage <- sapply(data, function(x) {
paste0(round(sum(is.na(x)) / length(x) * 100, digits = 2), "%")
})
return(nan_percentage)
}
# Calculate percentage of NaNs in each variable
nan_percentages <- calculate_nan_percentage(train)
# Print the percentage of NaNs for each variable
print(nan_percentages)
# Variables names are passed, with training and test set, to the function specified above.
#updated numerical variables names onto using mean-imputation
numerical_col_names<-c("age", "bmi", "ferritin","crprot","hemoglobin","gluc","ldl","hdl","trig","hba1c","systolic_bp","diastolic_bp")
# mode for other variables
names_var<-colnames(train)
categorical_col_names<-names_var[-which(names_var %in% c(numerical_col_names,'outcome'))]
imputed_data<-impute_mean_mode(train,test,numerical_col_names,categorical_col_names)
norm_data<-normalize_min_max(imputed_data$imputed_train,imputed_data$imputed_test)
train_imp_norm<-norm_data$norm_train
test_imp_norm<-norm_data$norm_test
nan_percentages <- calculate_nan_percentage(train_imp_norm)
print(nan_percentages)
nan_percentages <- calculate_nan_percentage(test_imp_norm)
print(nan_percentages)
log_full_model<-glm(outcome~., data = train_imp_norm, family="binomial")
print(summary(log_full_model))
# here probability are saved
full_model_prediction<-predict(log_full_model, type="response", newdata = test_imp_norm)
# funtion to handle the roc curve even in the next phases
roc_evaluation<-function(prob_prediction, plotting=TRUE,printing=FALSE, auc_out=FALSE){
library(pROC)
roc_full_model<-roc(response=test_imp_norm$outcome,predictor=prob_prediction)
#plotting roc curve
x<-1-roc_full_model$specificities
y<-roc_full_model$sensitivities
if(plotting){
plot(x,y,main=c('ROC curve with AUC of',toString(round(roc_full_model$auc,digits=3))),
xlab='1-specificity',ylab='sentivity',type='l')+lines(c(0,1),c(0,1),col="red")
}
if(printing){
cat('AUC of the full model', toString(round(roc_full_model$auc,digits=3)))
}
if(auc_out){
return(round(roc_full_model$auc,digits=3)) }
}
roc_evaluation(full_model_prediction,plotting=TRUE, printing = FALSE, auc_out=FALSE)
auc_full_model<-roc_evaluation(full_model_prediction,plotting=FALSE, printing = FALSE, auc_out=TRUE)
AUC_output<-rep(NA,200)
set.seed(0)
for(i in 1:200){
train_boo_c1_ind<-sample(which(train_imp_norm$outcome==1),length(which(train_imp_norm$outcome==1)),replace = TRUE)
train_boo_c0_ind<-sample(which(train_imp_norm$outcome==0),length(which(train_imp_norm$outcome==0)),replace=TRUE)
train_boo_ind<-sort(c(train_boo_c1_ind,train_boo_c0_ind))
train_boo<-train_imp_norm[train_boo_ind,]  # will act as internal train test
set_oob<-train_imp_norm[-train_boo_ind,]   # will act as internal test set
boo_full_model<-suppressWarnings(glm(outcome~., data = train_boo, family="binomial"))
boo_prob_prediction<-predict(boo_full_model, type="response", newdata = set_oob)
roc_boo_full_model<-roc(response=set_oob$outcome,predictor=boo_prob_prediction,quiet=TRUE)
AUC_boo<-roc_boo_full_model$auc
AUC_output[i]<-AUC_boo
}
boxplot(AUC_output,main='auc bootstrap')
# mean and quantile
mean_auc_full<-mean(AUC_output)
# 95% quantile
CI_low<-quantile(AUC_output,0.025)
CI_upper<-quantile(AUC_output,0.975)
cat('AUC of the full model', toString(auc_full_model),'\n----Bootstrap----','\nmean full model AUC',toString(round(mean_auc,digits=3)), '\nCI upper bound', toString(round(CI_upper,digits=3)),'\nCI lower bound', toString(round(CI_low,digits=3)))
# Setting the working directory
setwd("D:/Desktop/ANDREA/Universita/Magistrale/Anno Accademico 2023-2024/2. Medical Big Data/Final Project")
rm(list=ls())
load("data_final_project.RData")
# inspecting the database
summary(data)
# performing stratified splitting
# indices
pos_ind<-which(data$outcome==1)
neg_ind<-which(data$outcome==0)
set.seed(0)
# random splitting the data stratified
tr_ind_pos<-sample(pos_ind,size=round(0.8*length(pos_ind)),replace = FALSE)
tr_ind_neg<-sample(neg_ind,size=round(0.8*length(neg_ind)),replace = FALSE)
ind_train<-sort(c(tr_ind_neg,tr_ind_pos))
ind_test<-1:nrow(data)
ind_test<-ind_test[ - ind_train]
# train and test dataset
train<-data[ind_train,]
test<-data[ind_test,]
calc_perc<-function(df){
perc_0_tot<-nrow(df[df$outcome==0,])/nrow(df)*100
perc_1_tot<-nrow(df[df$outcome==1,])/nrow(df)*100
cat('    - Positive (diabetic) class:',toString(round(perc_1_tot),digits=3),'%\n')
cat('    - Negative (no diabetic) class:',toString(round(perc_0_tot),digits=3),'%\n')
}
cat('Whole df\n')
calc_perc(data)
cat('Train df\n')
calc_perc(train)
cat('Test df\n')
calc_perc(test)
rule_tumb_var<-nrow(train[train$outcome==1,])/ncol(train)
if(rule_tumb_var>=10){
cat('Rule of thumb verified: ',toString(round(rule_tumb_var, digits = 2)))
}
numerical_col_names<-c("age", "bmi", "waist", "ferritin","crprot","hemoglobin","gluc","ldl","hdl","tot_chol","trig","hba1c","systolic_bp","diastolic_bp")
num_train_df<- train[, numerical_col_names]
library(ggplot2)
col_names<-colnames(num_train_df)
for(i in 1:ncol(num_train_df)){
g<-ggplot(data=num_train_df, aes(y=num_train_df[,col_names[i]]))+
geom_boxplot(na.rm=TRUE)+
labs(title = toString(col_names[i]))
print(g)
}
library(corrplot)
corrplot(corr=cor(na.omit(num_train_df)), method = 'number')
train$waist<-NULL
test$waist<-NULL
train$tot_chol<-NULL
test$tot_chol<-NULL
impute_mean_mode <- function(train,test,var_by_mean,var_by_mode) {
mean_var_ind<-which(colnames(train) %in% var_by_mean)
for(k in mean_var_ind){
mean_var<-mean(train[,k],na.rm=TRUE)
nan_id<-which(is.na(train[,k]))
train[nan_id,k]<-mean_var
ind_na_test<-which(is.na(test[,k]))
test[ind_na_test,k]<-mean_var
}
mode_var_ind<-which(colnames(train) %in% var_by_mode)
for(k in mode_var_ind){
freq<-table(train[,k])
all_values<-sort(unique(train[,k]))
ind_mode<-which.max(freq)
mode_train_var<- all_values[ind_mode]
#imputing
nan_id<-which(is.na(train[,k]))
train[nan_id,k]<-mode_train_var
ind_na_test<-which(is.na(test[,k]))
test[ind_na_test,k]<-mode_train_var
}
return(list(imputed_train=train,imputed_test=test))
}
normalize_min_max<-function(train,test,var_to_norm){
ind_num<-which(sapply(train,is.factor)==FALSE)
for(k in ind_num){
min_by_k<-min(train[,k])
max_by_k<-max(train[,k])
# assign new variable values
delta_var<-max_by_k-min_by_k
train[,k]<-(train[,k]-min_by_k)/delta_var
test[,k]<-(test[,k]-min_by_k)/delta_var
}
return(list(norm_train=train,norm_test=test))
}
# Load required packages
library(dplyr)
# Function to calculate percentage of NaNs in each variable
calculate_nan_percentage <- function(data) {
nan_percentage <- sapply(data, function(x) {
paste0(round(sum(is.na(x)) / length(x) * 100, digits = 2), "%")
})
return(nan_percentage)
}
# Calculate percentage of NaNs in each variable
nan_percentages <- calculate_nan_percentage(train)
# Print the percentage of NaNs for each variable
print(nan_percentages)
# Variables names are passed, with training and test set, to the function specified above.
#updated numerical variables names onto using mean-imputation
numerical_col_names<-c("age", "bmi", "ferritin","crprot","hemoglobin","gluc","ldl","hdl","trig","hba1c","systolic_bp","diastolic_bp")
# mode for other variables
names_var<-colnames(train)
categorical_col_names<-names_var[-which(names_var %in% c(numerical_col_names,'outcome'))]
imputed_data<-impute_mean_mode(train,test,numerical_col_names,categorical_col_names)
norm_data<-normalize_min_max(imputed_data$imputed_train,imputed_data$imputed_test)
train_imp_norm<-norm_data$norm_train
test_imp_norm<-norm_data$norm_test
nan_percentages <- calculate_nan_percentage(train_imp_norm)
print(nan_percentages)
nan_percentages <- calculate_nan_percentage(test_imp_norm)
print(nan_percentages)
log_full_model<-glm(outcome~., data = train_imp_norm, family="binomial")
print(summary(log_full_model))
# here probability are saved
full_model_prediction<-predict(log_full_model, type="response", newdata = test_imp_norm)
# funtion to handle the roc curve even in the next phases
roc_evaluation<-function(prob_prediction, plotting=TRUE,printing=FALSE, auc_out=FALSE){
library(pROC)
roc_full_model<-roc(response=test_imp_norm$outcome,predictor=prob_prediction)
#plotting roc curve
x<-1-roc_full_model$specificities
y<-roc_full_model$sensitivities
if(plotting){
plot(x,y,main=c('ROC curve with AUC of',toString(round(roc_full_model$auc,digits=3))),
xlab='1-specificity',ylab='sentivity',type='l')+lines(c(0,1),c(0,1),col="red")
}
if(printing){
cat('AUC of the full model', toString(round(roc_full_model$auc,digits=3)))
}
if(auc_out){
return(round(roc_full_model$auc,digits=3)) }
}
roc_evaluation(full_model_prediction,plotting=TRUE, printing = FALSE, auc_out=FALSE)
auc_full_model<-roc_evaluation(full_model_prediction,plotting=FALSE, printing = FALSE, auc_out=TRUE)
AUC_output<-rep(NA,200)
set.seed(0)
for(i in 1:200){
train_boo_c1_ind<-sample(which(train_imp_norm$outcome==1),length(which(train_imp_norm$outcome==1)),replace = TRUE)
train_boo_c0_ind<-sample(which(train_imp_norm$outcome==0),length(which(train_imp_norm$outcome==0)),replace=TRUE)
train_boo_ind<-sort(c(train_boo_c1_ind,train_boo_c0_ind))
train_boo<-train_imp_norm[train_boo_ind,]  # will act as internal train test
set_oob<-train_imp_norm[-train_boo_ind,]   # will act as internal test set
boo_full_model<-suppressWarnings(glm(outcome~., data = train_boo, family="binomial"))
boo_prob_prediction<-predict(boo_full_model, type="response", newdata = set_oob)
roc_boo_full_model<-roc(response=set_oob$outcome,predictor=boo_prob_prediction,quiet=TRUE)
AUC_boo<-roc_boo_full_model$auc
AUC_output[i]<-AUC_boo
}
boxplot(AUC_output,main='auc bootstrap')
# mean and quantile
mean_auc_full<-mean(AUC_output)
# 95% quantile
CI_low<-quantile(AUC_output,0.025)
CI_upper<-quantile(AUC_output,0.975)
cat('AUC of the full model', toString(auc_full_model),'\n----Bootstrap----','\nmean full model AUC',toString(round(mean_auc_full,digits=3)), '\nCI upper bound', toString(round(CI_upper,digits=3)),'\nCI lower bound', toString(round(CI_low,digits=3)))
printing_sign_vars<-function(log_full_model){
summary_glm_full<-summary(log_full_model)
p_values <- summary_glm_full$coefficients[, "Pr(>|z|)"]
significant_vars <- names(p_values)[p_values < 0.05]
# printing
significant_p_values <- p_values[p_values < 0.05]
significant_results <- data.frame(Variable = significant_vars, p_value = significant_p_values)
print(significant_results)
}
printing_sign_vars(log_full_model)
# BOOSTRAP BACKWARD FEATURE SELECTION
stable_stpwise_approach<-function(train_imp_norm,seed_num,n_iter,stepwise_method, print_feature_ranking=TRUE){
feature_ranking<-c()
set.seed(seed_num)
for(i in 1:n_iter){
train_boo_c1_ind<-sample(which(train_imp_norm$outcome==1),length(which(train_imp_norm$outcome==1)),replace = TRUE)
train_boo_c0_ind<-sample(which(train_imp_norm$outcome==0),length(which(train_imp_norm$outcome==0)),replace=TRUE)
train_boo_ind<-sort(c(train_boo_c1_ind,train_boo_c0_ind))
# internal sets
train_boo<-train_imp_norm[train_boo_ind,]  # will act as internal train test
set_oob<-train_imp_norm[-train_boo_ind,]   # will act as internal test set
# boostrap feature selection
boo_full_model<-suppressWarnings(glm(outcome~., data = train_boo, family="binomial"))
boo_null_model<-suppressWarnings(glm(outcome~1, data = train_boo, family="binomial"))
model_range<- list(lower=formula(boo_null_model),upper=formula(boo_full_model))
# as the intent is comparing backward and stepwise backward, the function can be used as it is. Should be modified for other methods, in particulard to compare forward approaches.
boo_reduced_model<-suppressWarnings(step(object=boo_full_model,scope=model_range,direction=stepwise_method, trace=FALSE))
# saving the features
features<-colnames(boo_reduced_model$model)
features<-features[-which(features=='outcome')]
feature_ranking<-c(feature_ranking,features)
}
#Analysis of the results
abs_freq<-table(feature_ranking)
selection_percentage<-sort(abs_freq/50 * 100, decreasing = TRUE)
if(print_feature_ranking){print(selection_percentage)}
return(selection_percentage)
}
selection_percentage<-stable_stpwise_approach(train_imp_norm,seed_num=10,n_iter=30,stepwise_method='backward', print_feature_ranking=TRUE)
#selecting only the features which have the 55% of prevalence
final_features<-names(selection_percentage[which(selection_percentage>55)])
# how many features compared with other models?
cat('Number of features of the full model: ', toString(length(colnames(log_full_model$model))),'\nNumber of features of the boostrap backward reduced model: ', toString(length(final_features)),'\n')
# Final reduced model formula
final_model_formula_backward <-as.formula(paste("outcome", paste(final_features, collapse ="+"), sep='~'))
# final model
final_model_backward<-glm(final_model_formula_backward, data = train_imp_norm, family="binomial")
summary(final_model_backward)
# prediction and AUC saving
backward_pred<-predict(final_model_backward,type="response", newdata = test_imp_norm)
auc_backward_model<-roc_evaluation(backward_pred,plotting=FALSE, printing = FALSE, auc_out=TRUE)
selection_percentage<-stable_stpwise_approach(train_imp_norm,seed_num=10,n_iter=30,stepwise_method='both', print_feature_ranking=TRUE)
final_features<-names(selection_percentage[which(selection_percentage>55)])
# how many features compared with other models?
cat('Number of features of the full model: ', toString(length(colnames(log_full_model$model))),'\nNumber of features of the boostrap backward reduced model: ', toString(length(final_features)),'\n')
# Final reduced model formula
final_model_formula_stepback <-as.formula(paste("outcome", paste(final_features, collapse ="+"), sep='~'))
# final model
final_model_back_stepwise<-glm(final_model_formula_stepback, data = train_imp_norm, family="binomial")
summary(final_model_back_stepwise)
# prediction and AUC saving
stepback_pred<-predict(final_model_back_stepwise,type="response", newdata = test_imp_norm)
auc_stepback_model<-roc_evaluation(stepback_pred,plotting=FALSE, printing = FALSE, auc_out=TRUE)
# creation of the folds for cross validation
K=5
set.seed(10)
ind_0<-which(train_imp_norm$outcome==0)
ind_1<-which(train_imp_norm$outcome==1)
n0_per_fold<-round(length(ind_0)/K)
n1_per_fold<-round(length(ind_1)/K)
training_folds<-list()
validation_folds<-list()
for(k in 1:(K-1)){
ind_0_k<-sample(ind_0, n0_per_fold,replace=FALSE)
ind_1_k<-sample(ind_1, n1_per_fold,replace=FALSE)
ind_fold_k<-c(ind_1_k,ind_0_k)
training_folds[[k]]<-train_imp_norm[ind_fold_k,]
validation_folds[[k]]<-train_imp_norm[-ind_fold_k,]
# to really divide the dataset once selected a subject it cannot be selected again
ind_0<-setdiff(ind_0,ind_0_k)
ind_1<-setdiff(ind_1,ind_1_k)
}
# it remains the last fold so...
ind_fold_k<-c(ind_1,ind_0)
training_folds[[k+1]]<-train_imp_norm[ind_fold_k,]
validation_folds[[k+1]]<-train_imp_norm[-ind_fold_k,]
L=c(seq(0.001,0.01,by=0.0001),seq(0.02,1,by=0.001)) # possible values of Lambda
nL<-length(L)
auroc_matrix<-matrix(rep(NA,nL*K),nrow=nL,ncol=K)
library(glmnet)
for(k in 1:5){
internal_train<-training_folds[[k]]
internal_test<-validation_folds[[k]]
# matrix of internal train and test data to feed the model.
internal_training_matrix<-model.matrix(outcome~., internal_train)[,-1]
internal_testing_matrix<-model.matrix(outcome~., internal_test)[,-1]
for(j in 1:nL){
lambda=L[j]
# training LASSO
lasso_model<-glmnet(internal_training_matrix,internal_train$outcome, family='binomial', alhpa=1, lambda=lambda, standadize=F)
p<-predict(lasso_model, type="response", newx = internal_testing_matrix)
roc_boo<-roc(response=internal_test$outcome,as.vector(p),quiet=T)
auroc_matrix[j,k]<-roc_boo$auc
}
}
# mean AUC for lambda values
mean_AUC_lambda<-apply(auroc_matrix,1,mean)
# the best model will have the higher AUC on average, so the best lambda should allow to stay close to such maximum value. Let's say that is 0.005 from the mean
max_auroc<-max(mean_AUC_lambda)
auroc_limit<-max_auroc-0.005
ind<-which(mean_AUC_lambda>auroc_limit)
ind_opt<-ind[length(ind)] # best lambda will be positioned at the end of ind list because basically we take the maximum one
lambda_opt<-L[ind_opt]
cat('Value of lambda tuned with 5-folds-cross validation: ',toString(lambda_opt), '\nWith mean AUC of: ', toString(round(mean_AUC_lambda[ind_opt],digits = 3)))
final_lasso_model<-glmnet(internal_training_matrix,internal_train$outcome, family='binomial', alhpa=1, lambda=lambda_opt, standadize=F)
final_lasso_model$beta
lasso_prediction<-predict(final_lasso_model, type="response", newx = internal_testing_matrix)
roc_lasso<-roc(response=internal_test$outcome,as.vector(lasso_prediction),quiet=T)
auc_final_lasso<-round(roc_lasso$auc,digits = 3)
cat('\nFinal Lasso model AUC: ',toString(auc_final_lasso))
cat('AUC of the full model:',toString(auc_full_model),'is compared with\n','   - AUC backward reduced model:',toString(auc_backward_model),'\n','   - AUC stepwise backward reduced model:',toString(auc_stepback_model),'\n','   - AUC lasso model:',toString(auc_final_lasso))
summary(final_model_backward)
roc_evaluation(backward_pred,plotting=TRUE)
roc_evaluation(backward_pred,plotting=TRUE)
roc_evaluation(backward_pred,plotting=TRUE)
summary(final_model_backward)
setwd("D:/Desktop/ANDREA/Universita/Magistrale/Anno Accademico 2023-2024/TESI/Tesi_magistrale/Data/Processed/MAP_A")
rm(list=ls())
load("MAP_A1_refactored.csv")
data<-read.csv("MAP_A1_refactored.csv")
View(data)
View(data)
